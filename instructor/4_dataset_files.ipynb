{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a58ba4",
   "metadata": {},
   "source": [
    "(dkist:tutorial:dataset-exploring-files)=\n",
    "# Exploring Files in a `Dataset`\n",
    "\n",
    "In this chapter you will learn:\n",
    "\n",
    "- How to access dataset metadata from the FITS headers\n",
    "- How `Dataset` tracks FITS files and what to expect when slicing the data\n",
    "- How to obtain the quality report and preview movie for a dataset.\n",
    "\n",
    "First we need to re-create our dataset object from the last chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377fe1e",
   "metadata": {
    "tags": [
     "keep-inputs"
    ]
   },
   "outputs": [],
   "source": [
    "import dkist\n",
    "from dkist.data.sample import VISP_BKPLX\n",
    "\n",
    "ds = dkist.load_dataset(VISP_BKPLX)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad514e5",
   "metadata": {},
   "source": [
    "The `Dataset` object allows us to do some basic inspection of the dataset as a whole without having to download the entire thing, using the metadata in the FITS headers.\n",
    "This will save you a good amount of time and also ease the load on the DKIST servers.\n",
    "For example, we can check the seeing conditions during the observation and discount any data which will not be of high enough quality to be useful.\n",
    "We will go through this as an exercise in a later tutorial.\n",
    "\n",
    "## The `headers` table\n",
    "\n",
    "The FITS headers from every file in a dataset are duplicated and stored in the ASDF file.\n",
    "This means that all the metadata about each file is accessible using only the ASDF file before downloading any of the actual data.\n",
    "(It also means any changes you make to the headers in the headers table won't be reflected in the FITS files.)\n",
    "These headers are stored as a table in the `headers` attribute of the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c964513",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "ds.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773396e4",
   "metadata": {},
   "source": [
    "Since the headers are stored as a table, it is straightforward to inspect a keyword for all files at once.\n",
    "For example, to see the time of every frame in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129d6af",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "ds.headers['DATE-AVG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8113bcf",
   "metadata": {},
   "source": [
    "Alternatively if we want to look at all the columns but fewer files, we can either slice the table directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2742082",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.headers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792128b0",
   "metadata": {},
   "source": [
    "or we can slice the dataset to give us the files we want to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[:, 0].headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9edd9",
   "metadata": {},
   "source": [
    "The table is an instance of {obj}`astropy.table.Table`, and can therefore be inspected and manipulated in any of the usual ways for Table objects.\n",
    "Details of how to work with `Table` can be found on the astropy documentation.\n",
    "Notably though, columns can be used as arrays in many contexts.\n",
    "They can therefore be used for plotting, which allows us to visually inspect how metadata values vary over the many files in the dataset.\n",
    "For example, we might want to inspect the seeing conditions and plot the Fried parameter for all frames.\n",
    "\n",
    "First, if you're not familiar with all of the keywords in the header, they can be checked in the documentation.\n",
    "Helpfully, `Dataset` provides some additional metadata which includes a link to that documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.meta['inventory']['headerDocumentationUrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc539b",
   "metadata": {},
   "source": [
    "If you follow this link and then click on \"Level One FITS Specification\" you will find a list of all the FITS keywords used for level 1 data with a description of each.\n",
    "Using this we can find that the Fried parameter is stored with the keyword `\"ATMOS_R0\"`.\n",
    "Then it's trivial to plot this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ds[0].headers['ATMOS_R0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481eaca",
   "metadata": {},
   "source": [
    "Or we can use multiple columns from the headers to compare information or look at related values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f261a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.headers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c73f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "\n",
    "# Pick a better example here?\n",
    "plt.scatter(ds[0].headers[\"CRVAL1\"], ds[0].headers[\"CRVAL3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872dc13",
   "metadata": {},
   "source": [
    "## Tracking files\n",
    "\n",
    "`Dataset` tracks information about the individual files that make up the dataset in the `files` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ef43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba2366",
   "metadata": {},
   "source": [
    "This tells us that our (4, 425, 980, 2554) data array is stored as 1700 files, each containing an array of (1, 980, 2554).\n",
    "Since the filenames are automatically generated from observation metadata, the `files` attribute can also track those before we even download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e742feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ec49f",
   "metadata": {},
   "source": [
    "`Dataset` also knows the base path of the data - the path where the data is (or will be) saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files.basepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6e2e5",
   "metadata": {},
   "source": [
    "When we download the data for this dataset later on, this is where it will be saved.\n",
    "This defaults to the location of the ASDF file.\n",
    "Remember that there may be thousands of FITS files in a dataset, so in general you may want to use the path interpolation feature of the various download functions to keep your them arranged sensibly.\n",
    "This is why we have been downloading ASDF files to their own folders.\n",
    "\n",
    "We have mentioned already that slicing a dataset down to only the portion of it that interests us can be a way of reducing the size of the download once we want to actually get the data. We'll come back to both file tracking and downloads, but for now let us look at how our slicing operations impact the number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0998d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape, ds.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c9309",
   "metadata": {},
   "source": [
    "Here we can see that our initial starting point with the full dataset is an array of (4, 425, 980, 2554) datapoints stored in 1700 FITS files. Notice that the array in each file is of size (1, 980, 2554) - the dimensions match the spatial and dispersion axes of the data (with a dummy axis). Each file therefore effectively contains a single 2D image taken at a single raster location and polarization state, and many of these files put together make the full 4D dataset.\n",
    "\n",
    "Next let us slice our dataset as we did in the last chapter, and this time look at how that impacts the tracked files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d63121",
   "metadata": {},
   "outputs": [],
   "source": [
    "stokes_i = ds[0]\n",
    "stokes_i.data.shape, stokes_i.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e865",
   "metadata": {},
   "source": [
    "Since this slice only contains Stokes I, it only needs the files containing the corresponding data; those measured at the other polarization states have been dropped, leaving 425."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = ds[0, :, 200]\n",
    "scan.data.shape, scan.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728949f",
   "metadata": {},
   "source": [
    "In this case, however, although the dataset is obviously smaller it still spans the same 425 files. This is because we haven't sliced by raster location and are therefore taking one row of pixels from every file. To reduce the number of files any further we must look at fewer wavelengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ds[0, 100:200, :, 638:-628]\n",
    "feature.data.shape, feature.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c39a8",
   "metadata": {},
   "source": [
    "It is therefore important to pay attention to how your data are stored across files. As noted before, slicing sensibly can significantly reduce how many files you need to download, but it can also be a relevant concern when doing some computational tasks and when plotting, as every file touched by the data will need to be opened and loaded into memory.\n",
    "\n",
    "## Downloading the quality report and preview movie\n",
    "\n",
    "For each dataset a quality report is produced during calibration which gives useful information about the quality of the data.\n",
    "This is accessible through the `Dataset`'s `quality_report()` method, which will download a PDF of the quality report to the base path of the dataset.\n",
    "This uses parfive underneath, which is the same library `Fido` uses, so it will return the same kind of `results` object.\n",
    "If the download has been successful, this can be treated as a list of filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = ds.files.quality_report()\n",
    "qr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46c3ef",
   "metadata": {},
   "source": [
    "This method takes the optional arguments `path` and `overwrite`.\n",
    "`path` allows you to specify a different location for the download, and `overwrite` is a boolean which tells the method whether or not to download a new copy if the file already exists.\n",
    "\n",
    "Similarly, each dataset also has a short preview movie showing the data.\n",
    "This can be downloaded in exactly the same way as the quality report but using the `preview_movie()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = ds.files.preview_movie()\n",
    "pm"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
