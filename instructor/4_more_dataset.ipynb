{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f717616",
   "metadata": {},
   "source": [
    "(dkist:tutorial:more-dataset)=\n",
    "# More `Dataset`\n",
    "\n",
    "Firstly we need to re-create our dataset object from the last tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunpy.net import Fido, attrs as a\n",
    "import dkist\n",
    "import dkist.net\n",
    "\n",
    "res = Fido.search(a.dkist.Dataset('BKPLX'))\n",
    "files = Fido.fetch(res, path=\"~/dkist_data/{instrument}_{dataset_id}\")\n",
    "ds = dkist.load_dataset(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b0e17",
   "metadata": {},
   "source": [
    "The `Dataset` object allows us to do some basic inspection of the dataset as a whole without having to download the entire thing, using the metadata in the FITS headers.\n",
    "This will save you a good amount of time and also ease the load on the DKIST servers.\n",
    "For example, we can check the seeing conditions during the observation and discount any data which will not be of high enough quality to be useful.\n",
    "We will go through this as an exercise in a later tutorial.\n",
    "\n",
    "## The `headers` table\n",
    "\n",
    "The FITS headers from every file in a dataset are duplicated and stored in the ASDF file.\n",
    "This means that all the metadata about each file is accessible using only the ASDF file before downloading any of the actual data.\n",
    "(It also means any changes you make to the headers in the headers table won't be reflected in the FITS files.)\n",
    "These headers are stored as a table in the `headers` attribute of the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c620dfc",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "ds.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ec0a7",
   "metadata": {},
   "source": [
    "Since the headers are stored as a table, it is straightforward to inspect a keyword for all files at once.\n",
    "For example, to see the time of every frame in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefee8c",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "ds.headers['DATE-AVG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b6478",
   "metadata": {},
   "source": [
    "Or we can look at one or more rows (i.e.: files) by slicing the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.headers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d68e36",
   "metadata": {},
   "source": [
    "The table is an instance of {obj}`astropy.table.Table`, and can therefore be inspected and manipulated in any of the usual ways for Table objects.\n",
    "Details of how to work with `Table` can be found on the astropy documentation.\n",
    "Notably though, columns can be used as arrays in many contexts.\n",
    "They can therefore be used for plotting, which allows us to visually inspect how metadata values vary over the many files in the dataset.\n",
    "For example, we might want to inspect the seeing conditions and plot the Fried parameter for all frames.\n",
    "\n",
    "First, if you're not familiar with all of the keywords in the header, they can be checked in the documentation.\n",
    "Helpfully, `Dataset` provides some additional metadata which includes a link to that documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9abd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.meta['inventory']['headerDocumentationUrl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c67ee0",
   "metadata": {},
   "source": [
    "If you follow this link and then click on \"Level One FITS Specification\" you will find a list of all the FITS keywords used for level 1 data with a description of each.\n",
    "Using this we can find that the Fried parameter is stored with the keyword `\"ATMOS_R0\"`.\n",
    "Then it's trivial to plot this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd66d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ds.headers['ATMOS_R0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bd8bf",
   "metadata": {},
   "source": [
    "## Tracking files\n",
    "\n",
    "`Dataset` tracks information about the individual files that make up the dataset in the `files` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12749295",
   "metadata": {},
   "source": [
    "This tells us that our (20, 4096, 4096) data array is stored as 20 files, each containing an array of (4096, 4096).\n",
    "Since the filenames are automatically generated from observation metadata, the `files` attribute can also track those before we even download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69062912",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c4eda",
   "metadata": {},
   "source": [
    "`Dataset` also knows the base path of the data - the path where the data is (or will be) saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.files.basepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da278222",
   "metadata": {},
   "source": [
    "When we download the data for this dataset later on, this is where it will be saved.\n",
    "So in general you may want to use the path interpolation feature of the various download functions to keep your data files arranged sensibly, as we have for this example.\n",
    "\n",
    "### Reducing file downloads\n",
    "We have mentioned a few times already that we can reduce the size of a data download by finding just the data that interests us and slicing the dataset down to just that portion.\n",
    "However, there is an important point to note about this, which is that you need to keep in mind how the data are stored across the dataset's files.\n",
    "\n",
    "As we saw before, each FITS file contains effectively a 2D image - a single raster scan at one polarisation state - and we have many of these files to make a full 4D dataset.\n",
    "What this means is that if we look at a subset of the scan steps or polarisation states, we will reduce the number of files across which the array is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a229109",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf4e82",
   "metadata": {},
   "source": [
    "First, notice that when we slice a `Dataset` like this, the output we get here shows us not just the updated array shape but also the updated dimensions.\n",
    "Because we're looking at a single polarisation state, that axis and the corresponding physical axis have been removed.\n",
    "\n",
    "Going back to the `files` attribute, we can see that we do indeed have fewer files for this subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0].files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faae280",
   "metadata": {},
   "source": [
    "However, if we decide we want to look at a single wavelength, we are taking a row of pixels from every single file.\n",
    "So although we reduce the dimensions of the array, we are not reducing the number of files we need to reference - and therefore download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[:, :, 500, :].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[:, :, 500, :].files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a018c1f",
   "metadata": {},
   "source": [
    "## Downloading the quality report and preview movie\n",
    "\n",
    "For each dataset a quality report is produced during calibration which gives useful information about the quality of the data.\n",
    "This is accessible through the `Dataset`'s `quality_report()` method, which will download a PDF of the quality report to the base path of the dataset.\n",
    "This uses parfive underneath, which is the same library `Fido` uses, so it will return the same kind of `results` object.\n",
    "If the download has been successful, this can be treated as a list of filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb916b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = ds.files.quality_report()\n",
    "qr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2883e12",
   "metadata": {},
   "source": [
    "This method takes the optional arguments `path` and `overwrite`.\n",
    "`path` allows you to specify a different location for the download, and `overwrite` is a boolean which tells the method whether or not to download a new copy if the file already exists.\n",
    "\n",
    "Similarly, each dataset also has a short preview movie showing the data.\n",
    "This can be downloaded in exactly the same way as the quality report but using the `preview_movie()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = ds.files.preview_movie()\n",
    "pm"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
