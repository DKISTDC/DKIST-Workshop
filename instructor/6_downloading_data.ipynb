{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70080ec",
   "metadata": {},
   "source": [
    "(dkist:tutorial:downloading-data)=\n",
    "# Downloading Data\n",
    "\n",
    "## Setting up and using Globus\n",
    "\n",
    "We've seen how to search and download the ASDF metadata files with Fido.\n",
    "However, the actual data files are distributed using [Globus](https://www.globus.org/data-transfer).\n",
    "For this chapter you will need to be running Globus Connect Personal, so follow the installation instructions for your platform [here](https://www.globus.org/globus-connect-personal) if you haven't already.\n",
    "During the setup, you will need to login to Globus.\n",
    "For this you can use your login for your institution, or alternatively you can login with Google or ORCID.\n",
    "\n",
    "Once Globus is installed and set up, you will need to run Globus Connect Personal (GCP) as described on the installation page.\n",
    "You will need to do this every time you want to download data, either through the user tools or through the Globus web app.\n",
    "When you start GCP You may also want to define the location or locations on your computer which you want Globus to have access to.\n",
    "On Linux you can do this using the `-restrict-paths` command line argument, or by editing the config file.\n",
    "On Windows and Mac OS this option is in the \"Access\" tab of the configuration options.\n",
    "Globus will only be able to transfer files onto your machine in the specified paths.\n",
    "\n",
    "Globus will send an email to your registered email address when a download task finishes.\n",
    "Of course this is unnecessary for some of the more trivial examples in this tutorial, but if you're transferring a whole large dataset it will likely take some time to download and it may be useful to be notified when it's complete.\n",
    "Remember that you do need to have GCP running for the transfer to continue - so if you stop it then your data download will stop as well.\n",
    "\n",
    "If you try transferring the same file a second time to the same location, you will find that the task completes successfully but the file is not actually transferred.\n",
    "This is to save download time and avoid duplication.\n",
    "\n",
    "Finally, a couple of things to note on terminology:\n",
    "\n",
    "- **Endpoints** (also called **Collections** in the web app) are locations registered with Globus for data transfer.\n",
    "Many institutions will have their own Globus endpoints, such as a computing cluster, that you may have access to.\n",
    "DKIST has an endpoint called \"DKIST Data Transfer\", which is where DKIST data will be made available.\n",
    "The user tools use this as the default remote endpoint, and define a local endpoint on your current machine.\n",
    "\n",
    "- **Paths** Paths in Globus are broadly what you expect them to be.\n",
    "However, note that the paths are as the Globus endpoint sees them, so might not be identical to how you refer to them on your local system.\n",
    "\n",
    "### The Globus web app\n",
    "\n",
    "You may already be familiar with using the [Globus web app](https://app.globus.org/) to download data.\n",
    "If you are not, you should read through the [getting started docs here](https://docs.globus.org/how-to/get-started/).\n",
    "However, given the quantities of data that DKIST provides, we recommend using the user tools as your primary way to download data for science, and that is the method that will be covered in this chapter of the tutorial.\n",
    "In any case, the underlying concepts described above are the same.\n",
    "\n",
    "## Dataset and downloading\n",
    "\n",
    "For this section we don't recommend that you run the download commands as we go through the workshop unless you're willing to wait for them to complete, which may take some time.\n",
    "First let's load the VISP dataset we were using before.\n",
    "This time we'll download the ASDF using Fido rather than loading the sample data.\n",
    "This is so that we can continue using a dataset we're familiar with but also go through the full process of downloading both the ASDF and data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9743e11",
   "metadata": {
    "tags": [
     "keep-inputs"
    ]
   },
   "outputs": [],
   "source": [
    "import dkist\n",
    "import dkist.net\n",
    "from sunpy.net import Fido, attrs as a\n",
    "\n",
    "res = Fido.search(a.dkist.Dataset(\"BKPLX\"))\n",
    "f = Fido.fetch(res, path=\"~/sunpy/data/{instrument}/{dataset_id}/\")\n",
    "ds = dkist.load_dataset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c6234",
   "metadata": {},
   "source": [
    "As we saw earlier, we can use the `files` attribute to access information about the number and names of files in the dataset even before downloading any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.files)\n",
    "print(ds.files.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad08c13",
   "metadata": {},
   "source": [
    "The `files` attribute has a `download()` method that we will use for downloading the data.\n",
    "In order to speed up this demonstration a bit, we will just download the first file.\n",
    "To do this we can slice the dataset so that we're only accessing the portion of the data saved in the first file, paying attention to the chunking information in the `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358058a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0, 0].files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e245e",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ds[0, 0].files.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e91af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0, 0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514265c0",
   "metadata": {},
   "source": [
    "The default download directory used by `download()` is in the same folder as the ASDF file we loaded, so in this case `~/sunpy/data/VISP/BKPLX`.\n",
    "Since the `download()` method set up the transfer through globus, you can check on the status of your download in the activity tab of the web app as we saw earlier.\n",
    "\n",
    "We can change the download location of the files using the `path` argument.\n",
    "But remember that whatever path you specify must be accessible by Globus Connect Personal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19241b0d",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ds[0].files.download(path=\"~/somewhere/globus/can't/reach/\")  # will hang for a while and then fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4528d",
   "metadata": {},
   "source": [
    "The `path` keyword will replace placeholders in the path in the same way as we saw with `Fido.fetch()`.\n",
    "Any key in the dataset inventory (`ds.meta['inventory']`) can be used for this.\n",
    "So for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05fa21",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ds[0, 0].files.download(path=\"~/sunpy/data/{dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d81c80",
   "metadata": {},
   "source": [
    "would save the file to `~/sunpy/data/BKPLX/VISP_2023_10_16T18_21_47_508_00630200_I_BKPLX_L1.fits`.\n",
    "\n",
    "If we know that we will want to download an entire dataset, this can be done in the same way but using the full dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe56aa",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ds.files.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4278b78",
   "metadata": {},
   "source": [
    "Alternatively, the user tools offer another function which can also be used to download a full dataset.\n",
    "The `transfer_complete_datasets()` function can take a Fido search results object and download an entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5025620",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "results = Fido.search(a.Instrument(\"VISP\"), a.Time(\"2023-10-16 18:00\", \"2023-10-16 19:00\"))\n",
    "dkist.net.transfer_complete_datasets(results[\"dkist\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ef77d",
   "metadata": {},
   "source": [
    "Notice that we have to specify `results[\"dkist\"]` here, because `transfer_complete_datasets` only works for DKIST datasets, not any other kind of result that Fido might return.\n",
    "\n",
    "We can also download many datasets at once.\n",
    "For example if we have a proposal ID that we want to download all the data for we could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a10879",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "results = Fido.search(a.dkist.Proposal(\"pid_1_123\"))\n",
    "dkist.net.transfer_complete_datasets(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d756cd",
   "metadata": {},
   "source": [
    "This will iterate over the results and download each dataset in turn, with a progress bar for each.\n",
    "\n",
    "Of course, if this is a dataset you already know you will want to download all of - for example if it's your own observation - then you may not need to find it through Fido first.\n",
    "Fortunately, `transfer_complete_datasets()`, also lets you specify which datasets to download, by passing one or more dataset IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7d1d7",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "dkist.net.transfer_complete_datasets('BKPLX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d7164",
   "metadata": {},
   "source": [
    "Both `transfer_complete_datasets()` and `ds.files.download()` also allow you to specify remote endpoints using the `destination_endpoint` keyword.\n",
    "\n",
    "Normally both of these functions will block the terminal while the download is active.\n",
    "If you want to download a lot of data this is probably not useful, so you can turn this functionality off by passing `wait=False`.\n",
    "This will set up the transfer in Globus but then return from the function.\n",
    "Of course, be cautious with this approach if the next step of your code depends on the data being present.\n",
    "Setting `wait=False` will also skip the wait at the end of each dataset if downloading more than one, so all the transfers will be set up on Globus and then the function will return.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9ed36",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "results = Fido.search(a.dkist.Proposal(\"pid_1_123\"))\n",
    "dkist.net.transfer_complete_datasets(results, wait=False, progress=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
