{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "(dkist:tutorial:dataset-dimensionality)=\n",
    "# Dimensionality of a `Dataset`\n",
    "\n",
    "In this chapter you will learn how to open a dataset and inspect it, then choose a subset of the data to download.\n",
    "\n",
    "As previously discussed we know that a \"DKIST dataset\" is comprised of multiple files, including an ASDF and many FITS files.\n",
    "The Python tools represent all these files with the {obj}`dkist.Dataset` class.\n",
    "\n",
    "A `Dataset` object is constructed from the ASDF file for that dataset.\n",
    "This ASDF file contains the following information:\n",
    "* A table containing all the headers from all FITS files that comprise the dataset.\n",
    "* A copy of the Data Center's inventory record for the dataset.\n",
    "* A `gwcs` object which provides coordinate information for the whole dataset.\n",
    "* A list of all the component FITS files and the required order to combine them into a single array.\n",
    "\n",
    "In this chapter we will create `Dataset`s using only the ASDF files.\n",
    "This will mean we won't have access to the data arrays in the FITS files, but everything else will function the same.\n",
    "\n",
    "## DKIST Sample data\n",
    "\n",
    "We will come back to searching for ASDF files in a later chapter, when we cover Globus and downloading the FITS data files.\n",
    "For this chapter and the next few, we will use some sample datasets which are provided with the `dkist` package.\n",
    "These sample datasets provide convenient examples to work with and contain some FITS files which do not require Globus to access.\n",
    "They can be imported from `dkist.data.sample` and for this chapter we will use the VISP one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This constant defines the path to a folder containing the metadata ASDF and a few data files for a small VISP dataset.\n",
    "These are automatically downloaded as a .tar file and unpacked the first time you import the sample data so that the dataset is always available.\n",
    "\n",
    "## Constructing `Dataset` Objects\n",
    "\n",
    "The Python tools provide a utility function {obj}`dkist.load_dataset` which loads an ASDF file and creates one or more {obj}`dkist.Dataset` or {obj}`dkist.TiledDataset` objects.\n",
    "This function takes one of several different kinds of input:\n",
    "\n",
    "- a string representation of either; a valid ASDF file or a directory containing one;\n",
    "- a `pathlib.Path` object representing a valid ASDF or directory containing one;\n",
    "- a `parfive.results.Results` object as returned by `sunpy.Fido.fetch` (this will only work if _all_ results in the table are valid DKIST ASDF files); or\n",
    "- a list or tuple of any combination of the above.\n",
    "\n",
    "Since we have a `Path` to our sample data as defined above, we can pass this straight to `load_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Now we have a {obj}`dkist.Dataset` object which describes the shape, size and physical dimensions of the array, but doesn't assume the presence of any of the actual data.\n",
    "This may sound unhelpful but we'll see how it can be very powerful.\n",
    "\n",
    "First let's have a look at the basic representation of the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "This tells us that we have a 4-dimensional data cube and what values the axes correspond to.\n",
    "Importantly, it not only gives us information about the *pixel* axes (the actual dimensions of the array itself), but also the *world* axes (the physical quantities related to the observation).\n",
    "It also gives us a correlation matrix showing how the pixel axes relate to the world axes.\n",
    "\n",
    "## `Dataset` and `NDCube`: Coordinate aware arrays\n",
    "\n",
    "The `Dataset` class is an extension to [SunPy's `NDCube` class](https://docs.sunpy.org/projects/ndcube/).\n",
    "In this section we shall demonstrate some of the key functionality of `NDCube` with DKIST data.\n",
    "\n",
    "### Pixel, Array and World Ordering\n",
    "\n",
    "Before we jump into using the `Dataset` class we need to clarify some definitions:\n",
    "\n",
    "* **Pixel** ordering is defined as \"Cartesian\" ordering, or the same as Fortran ordering or column major. This is the ordering used by FITS files and WCS objects in Python.\n",
    "* **Array** ordering is defined as C or row-major ordering. This is used by Python's numpy arrays, as Python is implemented in C.\n",
    "* **World** coordinates are the physical coordinates that correspond to pixel coordinates. These are not always in either pixel or array order, but tend to be close to pixel order.\n",
    "\n",
    "The pixel grid will always be aligned with the array, so pixel and array coordinates are the same except for the ordering.\n",
    "\n",
    "### Coordinates, Arrays, Pixels, oh my!\n",
    "\n",
    "A key aspect of the `Dataset` is that it is coordinate aware.\n",
    "That is, it is able to map between array indices and physical dimensions.\n",
    "This means that you can easily convert from a position in the array to a location defined by physical coordinates.\n",
    "\n",
    "The first thing we can inspect about our dataset is the dimensionality of the underlying array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "These are the **array** dimensions.\n",
    "We can get the corresponding **pixel** axis names with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "note how these are reversed from one another, we can print them together with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "These axes map onto world axes via the axis correlation matrix as we saw above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can get a list of the world axes which correspond to each array axis with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This property is a list with one element for every array axis containing a tuple of the world axis physical types of each world axis correlated with that array axis.\n",
    "So the first array axis has one physical type `'phys.polarization.stokes'` but the second has three (lat, lon and time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Using the WCS object, we can convert between pixel or array coordinates and world coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "and we can also do the reverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Finally, it's possible to get the coordinates corresponding to each world axis using {obj}`~dkist.Dataset.axis_world_coords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This method (when called without any arguments) returns a tuple of coordinate objects covering all the different world axes, in this case it returns a `SkyCoord` for the two spatial axes, a `SpectralCoord` for the spectral axis, a `Time` object for the temporal axis and a `StokesCoord` for the Stokes axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "It's possible to pass an argument to `axis_world_coords` which is one or more world axis physical types (or unique substrings thereof) so this would just return the coordinates along the temporal world axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Slicing Datasets\n",
    "\n",
    "Another useful feature of the `Dataset` class, which it inherits from `NDCube`, is the ability to \"slice\" the dataset and return a smaller dataset, with the array and coordinate information in tact.\n",
    "The syntax for this is exactly as you would expect for a NumPy array, but it is worth taking quick look at how the coordinates are handled.\n",
    "\n",
    "For example, to extract the Stokes I component of the dataset we would access the first item of the first axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "And to get a full raster scan in Stokes I at a single wavelength we would further index the dispersion axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "First, notice that when we slice a Dataset like this, the output we get here shows us not just the updated array shape but also the updated dimensions. Because we're looking at a single polarisation state and a single wavelength, those axes and the corresponding world axes have been removed. This gives us a new dataset with the smaller 425x2554 pixel dimensions and the world axes associated with those. The relationship between the remaining pixel and world axes stays the same as shown in the correlation matrix.\n",
    "\n",
    "You may notice that we have \"lost\" some information here. Having dropped two world axes we no longer have the coordinate information for them, and so cannot tell what world coordinates the new dataset corresponds to. However, this information is preserved in the dataset, just not in the axes. There is a `.global_coords` attribute, which contains coordinate information applicable to the whole dataset. In this case that would be the wavelength and the Stokes parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Again following usual NumPy slicing syntax, we can also slice out a section of an axis of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Notice again that this has reduced the dimensionality of the world coordinates as well as of the data itself."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
